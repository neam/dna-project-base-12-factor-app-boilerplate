Deploy to Tutum
===============

Background information and complementary documentation available in `vendor/neam/yii-dna-deployment/README.md`

## Build server first time setup

On build server:

    cd ~
    git clone -b develop git@bitbucket.org:_PROJECT_/_PROJECT_-project.git
    cd ~/_PROJECT_-project
    git clone --recursive -b develop git@bitbucket.org:_PROJECT_/_PROJECT_-product.git _PROJECT_-product
    cd ~/_PROJECT_-project/_PROJECT_-product

Locally:

    scp .env _PROJECT_@build._PROJECT_.com:/home/dokku/_PROJECT_-project/_PROJECT_-product/.env
    scp .current-local-cli-data-profile _PROJECT_@build._PROJECT_.com:/home/dokku/_PROJECT_-project/_PROJECT_-product/.current-local-cli-data-profile
    scp deploy/config/deploy-prepare-secrets.php _PROJECT_@build._PROJECT_.com:/home/dokku/_PROJECT_-project/_PROJECT_-product/deploy/config/deploy-prepare-secrets.php
    scp deploy/config/secrets.php _PROJECT_@build._PROJECT_.com:/home/dokku/_PROJECT_-project/_PROJECT_-product/deploy/config/secrets.php

On build server:

    cd ~/_PROJECT_-project/_PROJECT_-product
    stack/start.sh
    stack/restart.sh
    docker-compose run -e PREFER=dist builder stack/src/install-deps.sh
    vendor/bin/docker-stack build-directory-init


## General deployment routine

First, make sure that everything is tested, committed and pushed. 

Create a new release in SourceTree.

Push the new branch. 

Open up two new terminal windows/tabs (yes, they need to be freshly created so that no old environment variables are hanging around).

In one of them, connect to the build server:

    ssh _PROJECT_@build._PROJECT_.com
    cd ~/_PROJECT_-project/_PROJECT_-product

In both terminals, run the following:

    export DATA=%DATA% # change to the deployment that you wish to deploy. for a multi-tenant deployment, use the subdomain including "%DATA%", ie "%DATA%.player" or simply "%DATA%" to first-level subdomains
    export BRANCH_TO_DEPLOY=""
    export COMMITSHA=""
    source deploy/prepare.sh

Then, on the build server:

    cd ~/_PROJECT_-project/_PROJECT_-product
    stack/src/git-pull-recursive.sh
    export BRANCH_TO_DEPLOY=""
    export COMMITSHA=""
    source deploy/prepare.sh

Now check that both terminal windows show the same value for COMMITSHA. This is critical. If not, you may be on different branches, switch branch:

    git branch -r # to display available branches
    git checkout <the-branch>
    export BRANCH_TO_DEPLOY=""
    export COMMITSHA=""
    source deploy/prepare.sh

Then, on the build server, run:

    vendor/bin/docker-stack build-directory-sync # (don't worry about "fatal: Cannot force update the current branch", it is expected)
    cd ../$(basename $(pwd))-build/

Set up a temporary deployment on the build server - Part 1:

    stack/start.sh
    stack/restart.sh

Then, on the build server, run:

    docker-compose run -e PREFER=dist builder stack/src/install-deps.sh

Set up a temporary deployment on the build server - Part 2:

    export DATA=clean-db
    echo "DATA=$DATA" >> .current-local-cli-data-profile
    docker-stack local run worker /bin/bash bin/ensure-db.sh
    docker-stack local run worker /bin/bash bin/reset-db.sh

Or:

    export DATA=example # change to a specific data profile that you would like to test
    echo "DATA=$DATA" >> .current-local-cli-data-profile
    docker-stack local run worker /bin/bash bin/ensure-db.sh
    docker-stack local run worker /bin/bash bin/reset-db.sh --force-s3-sync
    stack/src/set-writable-local.sh

Now we need to test the build. The base URL is generated by the following command:

    docker-stack local url router 80 $DATA._PROJECT_.build._PROJECT_.com

When you have verified that everything works, build and push the source code to tutum:
    
    vendor/neam/yii-dna-deployment/deploy/build.sh

Then, locally (the other terminal window):

    deploy/generate-config.sh
    
Follow the instructions printed by the command. When it is time to follow "Then, run one of the following to deploy", run the first group of commands (tutum stack create and start). 

This will start a second, parallel, stack for the campaign if one was already there before. 

Wait for it to be fully running.

Copy the stack's name (in the style <date><vhost><commitsha>) and run the following:

    deploy/diagnose.sh <stackname>

This will amongst other things list something like the following:

    # Health-checks for first-level backend (Nginx):
    export WEB_PORT=49232
    export WEB_FQDN=cfa9591a-foo.node.tutum.io
    stack/_util/health-checks.sh

This gives the address to where the stack is running before it is connected to the public campaign domain name (we will do that later). To check that things work as expected:

    open http://$WEB_FQDN:$WEB_PORT/internal/cms/
    
Also, add //$WEB_FQDN:$WEB_PORT to relevant Userapp-users settings in order to verify that the REST api works as expected.

If you need to upload files in the backend, reset the db or upload new files to the live cdn, you need to open a shell in the deployed phpfiles container. See "Running worker commands in an already deployed stack" below for instructions on how to do that.

When the new stack is verified to work as expected on `http://$WEB_FQDN:$WEB_PORT/`, you should link the new stack's web* service to the public router service in tutum (and remove any previous linked web*-service for that deployment) and then re-deploy the production router service so that it is receiving traffic to it's public domain name.

Until cache busting is implemented, we need to login to Cloudflare, visit the domain name of the updated campaign, choose Cache, and then Purge everything. Also, don't forget to tell every returning visitor to clear their cache...

The production release is then complete. 

If something is found to be wrong with the new stack, you may want to link back the previously working stack and re-deploy the production router service, or deploy a new stack with a fix.

If everything is ok, then go to source tree and finish the release using git flow + push the master and develop branches.

Also, don't forget to upload the deployment metadata to the build server so that we can all access it:

    scp -r deployments/* _PROJECT_@build._PROJECT_.com:/home/dokku/_PROJECT_-project/_PROJECT_-product/deployments/

## Caveats

When re-deploying the production router service, ALL stacks [will stop responding for 20-40 seconds](https://github.com/tutumcloud/tutum-docker-clusterproxy/issues/38). This will hopefully be resolved soon enough.

## Running worker commands in an already deployed stack

If you don't have the corresponding stack metadata (in the deployments/ directory), get it from the build server where the metadata about the deployments is stored:
    
    scp -r _PROJECT_@build._PROJECT_.com:/home/dokku/_PROJECT_-project/_PROJECT_-product/deployments/* deployments/

Then, run:

    export DATA=example
    source deploy/prepare.sh
    deploy/diagnose.sh <path-to-deployment>
    
For instance:

    export DATA=example
    source deploy/prepare.sh
    deploy/diagnose.sh deployments/20150505174239foobarfoobarfoobar3ae0903/
    
In the output from the diagnose.sh script you will have a command similar to:

    tutum exec <container-id> /bin/bash # (phpfiles-1)

Run it in order to open a shell in the deployed phpfiles container. Cd into the app directory:

    cd /app

Then run relevant commands, for instance:

    bin/upload-current-files-to-cdn.sh # Sync to CDN
    bin/upload-current-user-data.sh # Create a backup of the current files
    
And even: (but be careful - live files and database! only do this on new deployments to install a database that was locally curated before)

    bin/reset-user-generated-files.sh --force-s3-sync
    bin/reset-db.sh --force-s3-sync

## Adding a new DATA profile to a deployed stack

Run the following worker commands in the deployed stack:

    export DATA=newprofile
    bin/create-new-data-profile.sh $DATA
    bin/ensure-db.sh
    bin/reset-db.sh

Note: If the DATA profile should be associated with a subdomain different from the actual data profile, you need to add the new virtual host and associated data profile to the virtual host data profile mapping environment variable `VIRTUAL_HOST_DATA_MAP`. For instance, add `customsubdomain.adoveo.com|foodataprofile`
